{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modelling on my data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # This is often necessary for the WordNet lemmatizer in newer NLTK versions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('dog.n.01'), Synset('frump.n.01'), Synset('dog.n.03'), Synset('cad.n.01'), Synset('frank.n.02'), Synset('pawl.n.01'), Synset('andiron.n.01'), Synset('chase.v.01')]\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "# Test loading WordNet\n",
    "print(wn.synsets('dog'))  # Should return a list of Synsets for \"dog\"\n",
    "\n",
    "# Test loading Stopwords\n",
    "print(stopwords.words('english')[:10])  # Print first 10 English stopwords\n",
    "\n",
    "# Test loading Open Multilingual WordNet\n",
    "#print(wn.__version())  # Check if OMW is loaded by printing the version or similar attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing \n",
    "def preprocess_text(text):\n",
    "    try:\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = tokenizer.tokenize(text.lower())  # Tokenize and convert to lower case\n",
    "        cleaned_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stopwords.words('english')]\n",
    "        return \" \".join(cleaned_tokens)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {text}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 414128 entries, 0 to 414127\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count   Dtype         \n",
      "---  ------   --------------   -----         \n",
      " 0   Date     414128 non-null  datetime64[ns]\n",
      " 1   Time     414128 non-null  object        \n",
      " 2   User     414128 non-null  object        \n",
      " 3   Content  414128 non-null  object        \n",
      "dtypes: datetime64[ns](1), object(3)\n",
      "memory usage: 12.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "df = pd.read_csv('../data/clean_data/combo_conversations_latest.csv', parse_dates=['Date'])\n",
    "df.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.64101659456889\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing to your text column (assuming it's named 'Content')\n",
    "t0=time.time()\n",
    "df['processed_content'] = df['Content'].apply(preprocess_text)\n",
    "t1=time.time()\n",
    "print((t1-t0)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6800 entries, 0 to 6799\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Date     6800 non-null   object\n",
      " 1   Time     6800 non-null   object\n",
      " 2   User     6800 non-null   object\n",
      " 3   Content  6800 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 212.6+ KB\n",
      "0.9337754845619202\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "import time\n",
    "t0=time.time()\n",
    "df_sampled = pd.read_csv('../data/clean_data/temporally_sampled_dataset.csv')\n",
    "df_sampled.info()\n",
    "# Apply preprocessing to your text column (assuming it's named 'Content')\n",
    "df_sampled['processed_content'] = df_sampled['Content'].apply(preprocess_text)\n",
    "t1=time.time()\n",
    "print((t1-t0)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1726590228.6581056"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare text for LDA analysis\n",
    "texts = [text.split() for text in df['processed_content']]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# LDA model\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, random_state=100, update_every=1, passes=10, alpha='auto', per_word_topics=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.029*\"one\" + 0.024*\"yes\" + 0.019*\"body\" + 0.017*\"year\" + 0.016*\"way\"')\n",
      "(1, '0.036*\"water\" + 0.028*\"1\" + 0.028*\"formula\" + 0.023*\"http\" + 0.020*\"coconut\"')\n",
      "(2, '0.039*\"like\" + 0.023*\"would\" + 0.020*\"think\" + 0.018*\"know\" + 0.018*\"people\"')\n",
      "(3, '0.030*\"raw\" + 0.023*\"get\" + 0.022*\"milk\" + 0.020*\"eat\" + 0.020*\"meat\"')\n",
      "(4, '0.029*\"juice\" + 0.029*\"aajonus\" + 0.028*\"diet\" + 0.025*\"primal\" + 0.019*\"use\"')\n"
     ]
    }
   ],
   "source": [
    "# Print the topics\n",
    "topics = lda_model.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (0, '0.029*\"one\" + 0.024*\"yes\" + 0.019*\"body\" + 0.017*\"year\" + 0.016*\"way\"')\n",
    "- (1, '0.036*\"water\" + 0.028*\"1\" + 0.028*\"formula\" + 0.023*\"http\" + 0.020*\"coconut\"')\n",
    "- (2, '0.039*\"like\" + 0.023*\"would\" + 0.020*\"think\" + 0.018*\"know\" + 0.018*\"people\"')\n",
    "- (3, '0.030*\"raw\" + 0.023*\"get\" + 0.022*\"milk\" + 0.020*\"eat\" + 0.020*\"meat\"')\n",
    "- (4, '0.029*\"juice\" + 0.029*\"aajonus\" + 0.028*\"diet\" + 0.025*\"primal\" + 0.019*\"use\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.012*\"primal\" + 0.011*\"diet\" + 0.009*\"aajonus\" + 0.008*\"water\" + 0.007*\"still\"')\n",
      "(1, '0.017*\"http\" + 0.009*\"com\" + 0.008*\"www\" + 0.006*\"think\" + 0.005*\"oh\"')\n",
      "(2, '0.036*\"raw\" + 0.017*\"cheese\" + 0.013*\"food\" + 0.012*\"juice\" + 0.011*\"meat\"')\n",
      "(3, '0.018*\"like\" + 0.012*\"good\" + 0.012*\"milk\" + 0.010*\"fat\" + 0.010*\"yes\"')\n",
      "(4, '0.016*\"get\" + 0.015*\"would\" + 0.012*\"meat\" + 0.011*\"time\" + 0.011*\"yeah\"')\n",
      "2.7585029602050782e-05\n"
     ]
    }
   ],
   "source": [
    "# Prepare text for LDA analysis\n",
    "texts = [text.split() for text in df_sampled['processed_content']]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# LDA model\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, random_state=100, update_every=1, passes=10, alpha='auto', per_word_topics=True)\n",
    "# Print the topics\n",
    "t0=time.time()\n",
    "topics = lda_model.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "t1=time.time()\n",
    "total=t1-t0\n",
    "print(total/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (0, '0.012*\"primal\" + 0.011*\"diet\" + 0.009*\"aajonus\" + 0.008*\"water\" + 0.007*\"still\"')\n",
    "# (1, '0.017*\"http\" + 0.009*\"com\" + 0.008*\"www\" + 0.006*\"think\" + 0.005*\"oh\"')\n",
    "# (2, '0.036*\"raw\" + 0.017*\"cheese\" + 0.013*\"food\" + 0.012*\"juice\" + 0.011*\"meat\"')\n",
    "# (3, '0.018*\"like\" + 0.012*\"good\" + 0.012*\"milk\" + 0.010*\"fat\" + 0.010*\"yes\"')\n",
    "# (4, '0.016*\"get\" + 0.015*\"would\" + 0.012*\"meat\" + 0.011*\"time\" + 0.011*\"yeah\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling in a difference way by getting the latest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>User</th>\n",
       "      <th>Content</th>\n",
       "      <th>processed_content</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-02-14</th>\n",
       "      <td>08:02</td>\n",
       "      <td>Silmavi</td>\n",
       "      <td>Hello!</td>\n",
       "      <td>hello</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-14</th>\n",
       "      <td>15:40</td>\n",
       "      <td>Silmavi</td>\n",
       "      <td>when you eat it alone?</td>\n",
       "      <td>eat alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-14</th>\n",
       "      <td>15:42</td>\n",
       "      <td>Silmavi</td>\n",
       "      <td>Good idea to eat it once wuth honey and once a...</td>\n",
       "      <td>good idea eat wuth honey alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-14</th>\n",
       "      <td>15:43</td>\n",
       "      <td>Silmavi</td>\n",
       "      <td>My kids eat it with the meal.They aren't at ho...</td>\n",
       "      <td>kid eat meal home btw meal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-14</th>\n",
       "      <td>15:43</td>\n",
       "      <td>Silmavi</td>\n",
       "      <td>I will do it during week end</td>\n",
       "      <td>week end</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Time     User                                            Content  \\\n",
       "Date                                                                            \n",
       "2019-02-14  08:02  Silmavi                                             Hello!   \n",
       "2019-02-14  15:40  Silmavi                             when you eat it alone?   \n",
       "2019-02-14  15:42  Silmavi  Good idea to eat it once wuth honey and once a...   \n",
       "2019-02-14  15:43  Silmavi  My kids eat it with the meal.They aren't at ho...   \n",
       "2019-02-14  15:43  Silmavi                       I will do it during week end   \n",
       "\n",
       "                         processed_content  \n",
       "Date                                        \n",
       "2019-02-14                           hello  \n",
       "2019-02-14                       eat alone  \n",
       "2019-02-14  good idea eat wuth honey alone  \n",
       "2019-02-14      kid eat meal home btw meal  \n",
       "2019-02-14                        week end  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the 'Date' column as the index of the DataFrame\n",
    "df.set_index('Date', inplace=True)\n",
    "# Sort the DataFrame by the index (Date)\n",
    "df.sort_index(inplace=True)\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 14429 entries, 2024-06-15 to 2024-09-14\n",
      "Data columns (total 4 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   Time               14429 non-null  object\n",
      " 1   User               14429 non-null  object\n",
      " 2   Content            14429 non-null  object\n",
      " 3   processed_content  14429 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 563.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# Define a more specific time range\n",
    "start_datetime = '2024-06-15'\n",
    "end_datetime = '2024-09-14'\n",
    "\n",
    "# Slice the DataFrame for this specific range\n",
    "sliced_df = df.loc[start_datetime:end_datetime]\n",
    "sliced_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.028*\"day\" + 0.027*\"diet\" + 0.023*\"primal\" + 0.019*\"yes\" + 0.018*\"2\"')\n",
      "(1, '0.017*\"good\" + 0.015*\"like\" + 0.011*\"aajonus\" + 0.010*\"time\" + 0.010*\"even\"')\n",
      "(2, '0.017*\"like\" + 0.014*\"lol\" + 0.011*\"shit\" + 0.010*\"look\" + 0.009*\"old\"')\n",
      "(3, '0.021*\"urine\" + 0.021*\"u\" + 0.017*\"http\" + 0.016*\"bath\" + 0.013*\"hot\"')\n",
      "(4, '0.035*\"raw\" + 0.025*\"cheese\" + 0.020*\"eat\" + 0.015*\"know\" + 0.015*\"milk\"')\n",
      "4.2227904001871747e-05\n"
     ]
    }
   ],
   "source": [
    "# Prepare text for LDA analysis\n",
    "texts = [text.split() for text in sliced_df['processed_content']]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# LDA model\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, random_state=100, update_every=1, passes=10, alpha='auto', per_word_topics=True)\n",
    "# Print the topics\n",
    "t0=time.time()\n",
    "topics = lda_model.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "t1=time.time()\n",
    "total=t1-t0\n",
    "print(total/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
