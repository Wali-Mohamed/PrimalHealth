{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ac4dbf-c284-43d4-bfd3-38c8603d3d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install accelerate\n",
    "#from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "#from openai import OpenAI\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e738819f-9611-4129-a4db-faac69906b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME']='/run/cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb89515c-2dc1-41f1-83b7-6315f78676f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "password=os.getenv(\"POSTGRES_PASSWORD\")\n",
    "print(password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "392daf62-e7af-4ca1-a979-403aa687366a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Error: connection to server at \"localhost\" (::1), port 7777 failed: fe_sendauth: no password supplied\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import os\n",
    "password=os.getenv(\"POSTGRES_PASSWORD\")\n",
    "print(password)\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"localhost\",\n",
    "        database=\"primal_health\",\n",
    "        user=\"postgres\",\n",
    "        password=password,\n",
    "        port=\"7777\"\n",
    "    )\n",
    "    print(\"Connection successful\")\n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f594ed-7e8a-4a9c-8abb-5f3c233dd6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "from psycopg2.extras import DictCursor\n",
    "from datetime import datetime, timezone\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "RUN_TIMEZONE_CHECK = os.getenv('RUN_TIMEZONE_CHECK', '1') == '1'\n",
    "\n",
    "TZ_INFO = os.getenv(\"TZ\", \"Europe/Berlin\")\n",
    "tz = ZoneInfo(TZ_INFO)\n",
    "\n",
    "\n",
    "def get_db_connection():\n",
    "    return psycopg2.connect(\n",
    "        host=os.getenv(\"POSTGRES_HOST\", \"postgres\"),\n",
    "        database=os.getenv(\"POSTGRES_DB\", \"course_assistant\"),\n",
    "        user=os.getenv(\"POSTGRES_USER\", \"your_username\"),\n",
    "        password=os.getenv(\"POSTGRES_PASSWORD\", \"your_password\"),\n",
    "    )\n",
    "\n",
    "\n",
    "def init_db():\n",
    "    conn = get_db_connection()\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\"DROP TABLE IF EXISTS feedback\")\n",
    "            cur.execute(\"DROP TABLE IF EXISTS conversations\")\n",
    "\n",
    "            cur.execute(\"\"\"\n",
    "                CREATE TABLE conversations (\n",
    "                    id TEXT PRIMARY KEY,\n",
    "                    question TEXT NOT NULL,\n",
    "                    answer TEXT NOT NULL,\n",
    "                    model_used TEXT NOT NULL,\n",
    "                    response_time FLOAT NOT NULL,\n",
    "                    relevance TEXT NOT NULL,\n",
    "                    relevance_explanation TEXT NOT NULL,\n",
    "                    prompt_tokens INTEGER NOT NULL,\n",
    "                    completion_tokens INTEGER NOT NULL,\n",
    "                    total_tokens INTEGER NOT NULL,\n",
    "                    eval_prompt_tokens INTEGER NOT NULL,\n",
    "                    eval_completion_tokens INTEGER NOT NULL,\n",
    "                    eval_total_tokens INTEGER NOT NULL,\n",
    "                    openai_cost FLOAT NOT NULL,\n",
    "                    timestamp TIMESTAMP WITH TIME ZONE NOT NULL\n",
    "                )\n",
    "            \"\"\")\n",
    "            cur.execute(\"\"\"\n",
    "                CREATE TABLE feedback (\n",
    "                    id SERIAL PRIMARY KEY,\n",
    "                    conversation_id TEXT REFERENCES conversations(id),\n",
    "                    feedback INTEGER NOT NULL,\n",
    "                    timestamp TIMESTAMP WITH TIME ZONE NOT NULL\n",
    "                )\n",
    "            \"\"\")\n",
    "        conn.commit()\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "def save_conversation(conversation_id, question, answer_data, timestamp=None):\n",
    "    if timestamp is None:\n",
    "        timestamp = datetime.now(tz)\n",
    "\n",
    "    conn = get_db_connection()\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\n",
    "                \"\"\"\n",
    "                INSERT INTO conversations \n",
    "                (id, question, answer, model_used, response_time, relevance, \n",
    "                relevance_explanation, prompt_tokens, completion_tokens, total_tokens, \n",
    "                eval_prompt_tokens, eval_completion_tokens, eval_total_tokens, openai_cost, timestamp)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                \"\"\",\n",
    "                (\n",
    "                    conversation_id,\n",
    "                    question,\n",
    "                    answer_data[\"answer\"],\n",
    "                    answer_data[\"model_used\"],\n",
    "                    answer_data[\"response_time\"],\n",
    "                    answer_data[\"relevance\"],\n",
    "                    answer_data[\"relevance_explanation\"],\n",
    "                    answer_data[\"prompt_tokens\"],\n",
    "                    answer_data[\"completion_tokens\"],\n",
    "                    answer_data[\"total_tokens\"],\n",
    "                    answer_data[\"eval_prompt_tokens\"],\n",
    "                    answer_data[\"eval_completion_tokens\"],\n",
    "                    answer_data[\"eval_total_tokens\"],\n",
    "                    answer_data[\"openai_cost\"],\n",
    "                    timestamp\n",
    "                ),\n",
    "            )\n",
    "        conn.commit()\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "def save_feedback(conversation_id, feedback, timestamp=None):\n",
    "    if timestamp is None:\n",
    "        timestamp = datetime.now(tz)\n",
    "\n",
    "    conn = get_db_connection()\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\n",
    "                \"INSERT INTO feedback (conversation_id, feedback, timestamp) VALUES (%s, %s, COALESCE(%s, CURRENT_TIMESTAMP))\",\n",
    "                (conversation_id, feedback, timestamp),\n",
    "            )\n",
    "        conn.commit()\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "def get_recent_conversations(limit=5, relevance=None):\n",
    "    conn = get_db_connection()\n",
    "    try:\n",
    "        with conn.cursor(cursor_factory=DictCursor) as cur:\n",
    "            query = \"\"\"\n",
    "                SELECT c.*, f.feedback\n",
    "                FROM conversations c\n",
    "                LEFT JOIN feedback f ON c.id = f.conversation_id\n",
    "            \"\"\"\n",
    "            if relevance:\n",
    "                query += f\" WHERE c.relevance = '{relevance}'\"\n",
    "            query += \" ORDER BY c.timestamp DESC LIMIT %s\"\n",
    "\n",
    "            cur.execute(query, (limit,))\n",
    "            return cur.fetchall()\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "def get_feedback_stats():\n",
    "    conn = get_db_connection()\n",
    "    try:\n",
    "        with conn.cursor(cursor_factory=DictCursor) as cur:\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT \n",
    "                    SUM(CASE WHEN feedback > 0 THEN 1 ELSE 0 END) as thumbs_up,\n",
    "                    SUM(CASE WHEN feedback < 0 THEN 1 ELSE 0 END) as thumbs_down\n",
    "                FROM feedback\n",
    "            \"\"\")\n",
    "            return cur.fetchone()\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "def check_timezone():\n",
    "    conn = get_db_connection()\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\"SHOW timezone;\")\n",
    "            db_timezone = cur.fetchone()[0]\n",
    "            print(f\"Database timezone: {db_timezone}\")\n",
    "\n",
    "            cur.execute(\"SELECT current_timestamp;\")\n",
    "            db_time_utc = cur.fetchone()[0]\n",
    "            print(f\"Database current time (UTC): {db_time_utc}\")\n",
    "\n",
    "            db_time_local = db_time_utc.astimezone(tz)\n",
    "            print(f\"Database current time ({TZ_INFO}): {db_time_local}\")\n",
    "\n",
    "            py_time = datetime.now(tz)\n",
    "            print(f\"Python current time: {py_time}\")\n",
    "\n",
    "            # Use py_time instead of tz for insertion\n",
    "            cur.execute(\"\"\"\n",
    "                INSERT INTO conversations \n",
    "                (id, question, answer, model_used, response_time, relevance, \n",
    "                relevance_explanation, prompt_tokens, completion_tokens, total_tokens, \n",
    "                eval_prompt_tokens, eval_completion_tokens, eval_total_tokens, openai_cost, timestamp)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                RETURNING timestamp;\n",
    "            \"\"\", \n",
    "            ('test', 'test question', 'test answer', 'test model', 0.0, 0.0, \n",
    "             'test explanation', 0, 0, 0, 0, 0, 0, 0.0, py_time))\n",
    "\n",
    "            inserted_time = cur.fetchone()[0]\n",
    "            print(f\"Inserted time (UTC): {inserted_time}\")\n",
    "            print(f\"Inserted time ({TZ_INFO}): {inserted_time.astimezone(tz)}\")\n",
    "\n",
    "            cur.execute(\"SELECT timestamp FROM conversations WHERE id = 'test';\")\n",
    "            selected_time = cur.fetchone()[0]\n",
    "            print(f\"Selected time (UTC): {selected_time}\")\n",
    "            print(f\"Selected time ({TZ_INFO}): {selected_time.astimezone(tz)}\")\n",
    "\n",
    "            # Clean up the test entry\n",
    "            cur.execute(\"DELETE FROM conversations WHERE id = 'test';\")\n",
    "            conn.commit()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        conn.rollback()\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "if RUN_TIMEZONE_CHECK:\n",
    "    check_timezone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6270b2-5b72-4920-9b9a-ee720dc5242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "from huggingface_hub import HfApi, login\n",
    "import os\n",
    "api_token = os.getenv(\"huggingface\")\n",
    "\n",
    "# Log in with your Hugging Face API token\n",
    "login(token=api_token)\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "pipeline(\"Hey how are you doing today?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f12105f-83ef-4e4a-8e4a-524c536f2c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\user\\Documents\\LLM Zoomcamp\\Project')\n",
    "print(os.getcwd())\n",
    "file_path = os.path.join(os.getcwd(), 'data\\clean_data\\documents.json')\n",
    "file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d139654-cb24-436f-9121-e96c30f4e0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'r') as file:\n",
    "    documents=json.load(file)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a00d70-c9dd-42bc-8386-e45106075db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fb7819-ec6e-46e8-932a-e6a615b9e0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "    api_key='ollama',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2be331-f94f-4d79-8e13-0f9566ab78b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model='phi3',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac58d1dd-2d99-41a4-bee1-0a70a424c56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm('how is weather in london?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfb4085-ce6f-4e3f-a99e-0d1b2876f122",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=3\n",
    "y=9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd39e8a-1bfd-4c70-b025-c65a751978e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x @ y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d325767-391a-417b-8ea8-41147a9ffdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "# Define matrices as lists\n",
    "A = [[1, 2], [3, 4]]\n",
    "B = [[5, 6], [7, 8]]\n",
    "\n",
    "# Matrix multiplication\n",
    "result = [[sum(a * b for a, b in zip(A_row, B_col)) for B_col in zip(*B)] for A_row in A]\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bba45c4-1dd8-409b-9b03-996b22c45b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.getenv(\"DATA_PATH\", \"../data/clean_data/final_data_with_IDs_new.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064abddc-6d30-4114-9980-949cae448f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c861b50-36b3-42ac-bc75-fbdb56b7b13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_path=os.getenv('Data_Path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6fff70-4ddd-4782-a875-c02e5ff2ef85",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b70d15-21be-4550-9dac-d4c5a150b4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['Data']='rttt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9d3c95-d0f4-4beb-b94b-f2eef1ce35be",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['Data']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e41dc0-11b1-4dad-a11a-733cb6068c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#from dotenv import load_dotenv\n",
    "\n",
    "os.environ['RUN_TIMEZONE_CHECK'] = '0'\n",
    "\n",
    "from db import init_db\n",
    "\n",
    "#load_dotenv()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Initializing database...\")\n",
    "    init_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953e7cd8-38fd-44c9-9a45-1514bf51f5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "project=os.getenv('Project_Path')\n",
    "import os\n",
    "current=Path(project)\n",
    "new=os.path.join(current, 'notebooks')\n",
    "print(new)\n",
    "os.chdir(new)\n",
    "print(os.getcwd())\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "from openai import OpenAI\n",
    "import minsearch\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Load documents\n",
    "file_path = current / 'data'/ 'clean_data' / 'documents.json'\n",
    "print(file_path)\n",
    "#file_path = os.path.join(os.getcwd(), r'data/clean_data/documents.json')\n",
    "with open(file_path, 'r') as file:\n",
    "    documents = json.load(file)\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "# Initialize the minsearch index\n",
    "index = minsearch.Index(\n",
    "    text_fields=['Chunked_Content'],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "index.fit(documents)\n",
    "\n",
    "def search(query):\n",
    "    boost = {\n",
    "        'Chunked_Content': 1.9366969407339725\n",
    "    }\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={},\n",
    "        boost_dict=boost,\n",
    "        num_results=10\n",
    "    )\n",
    "    return results\n",
    "\n",
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "    You're a primal health adviser. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "    Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "    QUESTION: {question}\n",
    "\n",
    "    CONTEXT: \n",
    "    {context}\n",
    "    \"\"\".strip()\n",
    "\n",
    "    context = \"\\n\".join([f\"Chunked_Content: {doc['Chunked_Content']}\" for doc in search_results])\n",
    "    \n",
    "    return prompt_template.format(question=query, context=context).strip()\n",
    "\n",
    "# def llm(prompt):\n",
    "#     response = client.chat.completions.create(\n",
    "#         model='gpt-3.5-turbo',\n",
    "#         messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "#     )\n",
    "#     return response.choices[0].message.content\n",
    "# def llm(prompt):\n",
    "#     response = client.chat.completions.create(\n",
    "#         model='gpt-4o',\n",
    "#         messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "#     )\n",
    "#     return response.choices[0].message.content\n",
    "\n",
    "def llm(prompt,model=\"llama3.2\"):\n",
    "    # Command to run Ollama with the desired model and prompt\n",
    "    command = [\"ollama\", \"run\", model, prompt]\n",
    "\n",
    "    # Run the command and capture the output\n",
    "    result = subprocess.run(command, capture_output=True, text=True, encoding='utf-8')\n",
    "\n",
    "    # Return the result\n",
    "    return result.stdout\n",
    "\n",
    "# Example usage\n",
    "response = query_ollama(\"What is the capital of France?\")\n",
    "print(response)\n",
    "def rag(query):\n",
    "    # Check if the user is asking who created the bot\n",
    "    if query.lower() in [\"who created you?\", \"who is your creator?\", \"who made you?\"]:\n",
    "        return \"I was created by W Mohamed.\"\n",
    "\n",
    "    # Otherwise, proceed with normal search and LLM response\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    time.sleep(1)  # Simulating a delay for the function to work\n",
    "    return answer\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6391cb-4065-4027-aae3-c81348a538ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "query='what causes belly fat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caf6307-a4c7-4116-a4cf-ac8f8c86637e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fb8c07-9981-46e7-9980-0df0d893b9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Eating raw animal fat may not be as healing as expected, and cooking the fat before digestion might be necessary, especially tough fat like pork belly.\n",
    "# Fermentation could be the oldest form of cooking, but dry aging and wet fermenting may have different microbial effects. High meat consumption may \n",
    "# not work for everyone, as it can lead to a nightmare experience and changes in taste. Ultimately, \n",
    "# the causes of belly fat may vary from person to person.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c7edcd-27e2-4795-b0bc-85c84a260a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "cwd = Path.cwd()\n",
    "goal_dir = cwd.parent.parent / \"my_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad1f5c1-f036-49d1-99ad-514ff786a4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91239ea-d842-4237-8237-5cbecdd75877",
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b966738c-afc0-411a-be6b-2385410a2be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40016dd1-b475-4325-b75b-49fb7e5e4120",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(os.getcwd(), data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af210d22-4984-4317-9b5a-e6baac58f823",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(r'C:\\Users\\user\\Documents\\LLM Zoomcamp\\Project\\data\\clean_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b431d5-c774-4788-8834-8861dbc6fcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c23f25-13bd-431d-9a49-adc5349aa996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def query_ollama(prompt,model=\"llama3.2\"):\n",
    "    # Command to run Ollama with the desired model and prompt\n",
    "    command = [\"ollama\", \"run\", model, prompt]\n",
    "\n",
    "    # Run the command and capture the output\n",
    "    result = subprocess.run(command, capture_output=True, text=True, encoding='utf-8')\n",
    "\n",
    "    # Return the result\n",
    "    return result.stdout\n",
    "\n",
    "# Example usage\n",
    "response = query_ollama(\"What is the capital of France?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1ab06b-2f4a-418e-b7cb-ba3be3c5931f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Set the model name to use LLaMA model (e.g., LLaMA 7B)\n",
    "model_name = \"decapoda-research/llama3.2:latest\"  # Example: LLaMA-7B model\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Text input\n",
    "input_text = \"What is the capital of France?\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text using the model\n",
    "outputs = model.generate(**inputs, max_length=50)\n",
    "\n",
    "# Decode and print the output\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc81097d-cb1a-4867-bcd6-b7f2662d9340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "input_text = \"translate English to German: How old are you?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd937af4-ca2e-4dd9-9324-8cf7c063e62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query='tell me about healthy food?'\n",
    "def llm_flan(prompt):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\",max_length=2048, truncation=True).input_ids\n",
    "    #outputs = model.generate(input_ids, max_length=3048, num_beams=4, early_stopping=True)\n",
    "    outputs = model.generate(\n",
    "        input_ids, \n",
    "        max_new_tokens=1024,  # Set max number of new tokens\n",
    "        num_beams=4,          # Keep beams, but consider reducing for diversity\n",
    "        do_sample=False,       # Use sampling instead of beam search\n",
    "        top_p=0.95,            # Use nucleus sampling for diversity\n",
    "        early_stopping=False,  # Allow the model to generate more tokens\n",
    "        top_k=50,\n",
    "        temperature=1\n",
    "    )\n",
    "    result=tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b791f519-b57d-4e92-800f-f9b1c6d291c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is not working\n",
    "def llm_flan(prompt, generate_params=None):\n",
    "    # Specify to use CPU\n",
    "    #device = torch.device(\"cpu\")\n",
    "    if generate_params is None:\n",
    "        generate_params = {}\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=generate_params.get(\"max_length\", 1000),\n",
    "        num_beams=generate_params.get(\"num_beams\", 5),\n",
    "        do_sample=generate_params.get(\"do_sample\", False),\n",
    "        temperature=generate_params.get(\"temperature\", 1.0),\n",
    "        top_k=generate_params.get(\"top_k\", 50),\n",
    "        top_p=generate_params.get(\"top_p\", 0.95),\n",
    "    )\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0032cf60-90de-4b01-a7d4-82aa559a38e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_flan('how to lose belly fat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db83652-fc1e-465c-81d4-ac7d986e0b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c31ee7c-5c1f-4dd7-be45-f85357b26412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # alternative method creating with adding combined\n",
    "# import lancedb\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# # Load a pre-trained model to generate text embeddings\n",
    "# model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# # Example documents\n",
    "# documents = [\n",
    "#     {'id': 0, 'content': 'This is the first document.', 'number of sentences': 5, 'number of words': 56},\n",
    "#     {'id': 1, 'content': 'This is the second document.', 'number of sentences': 5, 'number of words': 49}\n",
    "# ]\n",
    "\n",
    "# # Generate embeddings for the documents\n",
    "# contents = [doc['content'] for doc in documents]\n",
    "# embeddings = model.encode(contents)\n",
    "\n",
    "# # Restructure the data\n",
    "# data = [{\"text\": doc['content'], \"embedding\": embedding.tolist()} for doc, embedding in zip(documents, embeddings)]\n",
    "\n",
    "# # Create or open a LanceDB database\n",
    "# db = lancedb.connect(\"./lancedb\")\n",
    "\n",
    "# # Create the table and insert data\n",
    "# table = db.create_table(\"my_vector_table\", data=data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710c8782-d261-46bf-ac1a-76ac9ae9245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"google/gemma-7b-it\",\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device=\"cpu\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you? Please, answer in pirate-speak.\"},\n",
    "]\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.95\n",
    ")\n",
    "assistant_response = outputs[0][\"generated_text\"][-1][\"content\"]\n",
    "print(assistant_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb20461-1157-4bc5-bea2-94a11945be4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "model_path = 'openlm-research/open_llama_3b'\n",
    "# model_path = 'openlm-research/open_llama_7b'\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_path, torch_dtype=torch.float16, device_map='auto',\n",
    ")\n",
    "\n",
    "prompt = 'Q: What is the largest animal?\\nA:'\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "generation_output = model.generate(\n",
    "    input_ids=input_ids, max_new_tokens=32\n",
    ")\n",
    "print(tokenizer.decode(generation_output[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3693971-2da3-4552-a6c0-c35f68051c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mistral\n",
    "\n",
    "from huggingface_hub import HfApi, login\n",
    "import os\n",
    "api_token = os.getenv(\"huggingface\")\n",
    "\n",
    "# Log in with your Hugging Face API token\n",
    "login(token=api_token)\n",
    "# Authenticate Hugging Face API\n",
    "#api = HfApi()\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the Mistral 7B tokenizer and model from Hugging Face\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"  # The model's path on Hugging Face\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "# Define a function to generate text using the Mistral model\n",
    "def generate_text(prompt, max_new_tokens=256):\n",
    "    # Encode the prompt using the tokenizer\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate tokens\n",
    "    outputs = model.generate(inputs.input_ids, max_new_tokens=max_new_tokens)\n",
    "    \n",
    "    # Decode the tokens to generate the text\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Tell me about the benefits of healthy food.\"\n",
    "generated_output = generate_text(prompt, max_new_tokens=150)\n",
    "print(generated_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f3c822-37a9-4cec-a00d-66177e9a0362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#from dotenv import load_dotenv\n",
    "\n",
    "os.environ['RUN_TIMEZONE_CHECK'] = '0'\n",
    "\n",
    "from db import init_db\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Initializing database...\")\n",
    "    init_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bea9fa1-b08a-4782-b390-e8c6e8bf4b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
